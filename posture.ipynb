{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760e771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow #follow https://www.tensorflow.org/install/pip\n",
    "!pip install numba\n",
    "!pip install sklearn\n",
    "!pip install mediapipe\n",
    "!pip install tqdm\n",
    "!pip install seaborn\n",
    "\n",
    "!conda install graphviz\n",
    "!conda install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from numba import cuda\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Python libraries\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f88ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed data, to ensure that between runs, the results would not differ\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.experimental.numpy.random.seed(seed)\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = str(seed)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = str(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca755e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check we are running tensorflow in GPU mode.\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "            print(\"{} physical GPU and {} logical GPU\".format(len(gpus), len(logical_gpus)))\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b20dec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1106f4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load the data as files.\n",
    "\n",
    "raw_data = sklearn.datasets.load_files(os.getcwd() + r\"/data\", shuffle = False, random_state = seed)\n",
    "files = raw_data[\"filenames\"]\n",
    "targets = raw_data[\"target\"]\n",
    "\n",
    "class_names = np.array(raw_data[\"target_names\"])\n",
    "number_of_classes = len(class_names)\n",
    "label_map = {label:num for num, label in enumerate(class_names)} #{\"class_name\": 0, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6eedbf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Drawing Code and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63391c48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#A function to pad the data array with 0s\n",
    "def pad_array(input_array, new_shape):\n",
    "    first, second = np.shape(input_array)\n",
    "    output_array = np.zeros(new_shape)\n",
    "    output_array[0:first, 0:second] = input_array\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise function to normalise the keypoint data so it's easier for the network\n",
    "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(10, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mediapipe parameters and variables\n",
    "min_detect = 0.7 #non-default\n",
    "min_track = 0.7 #non-default\n",
    "segmentation = False #default\n",
    "model_complex = 1 #default\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcf6e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Add keypoint data to each frame\n",
    "def mediapipe_detection(image, pose):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = pose.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                              landmark_drawing_spec = mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6403a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract keypoint data to each frame\n",
    "def extract_keypoints(results):\n",
    "    if results.pose_landmarks:\n",
    "        output = np.array([[result.x, result.y, result.z, result.visibility] for result in results.pose_landmarks.landmark]).flatten()\n",
    "        return output\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60943e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forms a 3D array of (samples, timestep, keypoints_length) of all input samples\n",
    "def keypoint_maker(video_list, shape):\n",
    "    video_array = []\n",
    "    with mp_pose.Pose(min_detection_confidence = min_detect, min_tracking_confidence = min_track,\n",
    "                      model_complexity = model_complex, enable_segmentation = segmentation) as pose:\n",
    "        \n",
    "        for video in tqdm(video_list):\n",
    "            cap = cv2.VideoCapture(video)\n",
    "            keypoints_list = []\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                image, results = mediapipe_detection(frame, pose)\n",
    "                keypoints = extract_keypoints(results)\n",
    "                if keypoints is not None:\n",
    "                    keypoints_list.append(keypoints)\n",
    "                else:\n",
    "                    continue\n",
    "            video = np.stack(keypoints_list, axis = 0)\n",
    "            video = scaler.fit_transform(video)\n",
    "            video = pad_array(video, shape) #pads to largest video\n",
    "            video = np.expand_dims(video, axis=0) #Turns (frame_size, keypoints_length) into (1, frame_size, keypoints_length)\n",
    "            video_array.append(video) #Add current video to list\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        tensor = np.vstack(video_array)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fe260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to see all the frames as images in a folder to check them\n",
    "def save_frames(video_list):\n",
    "    frame_folder = \"/frames_\" + str(min_detect) + \"_\" + str(min_track) + \"_\" + str(segmentation) + \"_\" + str(model_complex) + \"/\"\n",
    "    frame_path = os.getcwd() + frame_folder\n",
    "    \n",
    "    if not os.path.exists(frame_path):\n",
    "        os.mkdir(frame_path)\n",
    "    \n",
    "    with mp_pose.Pose(min_detection_confidence = min_detect, min_tracking_confidence = min_track,\n",
    "                      model_complexity = model_complex, enable_segmentation = segmentation) as pose:\n",
    "        \n",
    "        for video in tqdm(video_list):\n",
    "            cap = cv2.VideoCapture(video)\n",
    "            count = 0\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                image, results = mediapipe_detection(frame, pose)\n",
    "                keypoints = extract_keypoints(results)\n",
    "                file_name = frame_path + \"/\" + str(Path(video).stem) + \"_\" + str(count) + \".jpeg\"\n",
    "                cv2.imwrite(file_name, image)  \n",
    "                count += 1\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c33083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives information on the videos such as FPS and the video with most frames.\n",
    "def get_video_frame_data(video_list):\n",
    "    frame_array = []\n",
    "    fps_array = []\n",
    "    with mp_pose.Pose(min_detection_confidence=min_detect, min_tracking_confidence=min_track,\n",
    "                      model_complexity = model_complex, enable_segmentation=segmentation) as pose:\n",
    "        for video in tqdm(video_list):\n",
    "            cap = cv2.VideoCapture(video)\n",
    "            frame_length = 0\n",
    "            number_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_array.append(number_of_frames)\n",
    "            fps_array.append(fps)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    longest_video = max(frame_array)\n",
    "    return frame_array, fps_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32810cc2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Splitting/Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01236ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02d51d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#First slipts the train and test\n",
    "train_files, test_files, train_targets, test_targets = model_selection.train_test_split(files, \n",
    "                                                                                        targets, \n",
    "                                                                                        test_size=1 - train_ratio, \n",
    "                                                                                        random_state = seed, \n",
    "                                                                                        stratify = targets)\n",
    "#Then splits the test into validation and test.\n",
    "valid_files, test_files, valid_targets, test_targets = model_selection.train_test_split(test_files,\n",
    "                                                                                        test_targets,\n",
    "                                                                                        test_size = test_ratio / (test_ratio + validation_ratio),\n",
    "                                                                                        random_state = seed,\n",
    "                                                                                        stratify = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful information\n",
    "video_data = get_video_frame_data(files)\n",
    "frame_limit = max(video_data[0])\n",
    "fps_array = video_data[1]\n",
    "fps_min = min(fps_array)\n",
    "fps_max = max(fps_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8deec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Takes a long time so commented out until final run\n",
    "save_frames(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful information printed\n",
    "print(frame_limit)\n",
    "print(fps_min)\n",
    "print(fps_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87a132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ensure each set has an equally split number of classes, otherwise network is potentially biased to certain classes.\n",
    "print(np.bincount(train_targets))\n",
    "print(np.bincount(valid_targets))\n",
    "print(np.bincount(test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56fdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set the shape and form the train, validation, and test sets of data as keypoint arrays\n",
    "\n",
    "shape = list((len(files), frame_limit, 132)) #number of videos, number of frames, number of keypoints\n",
    "x_train = keypoint_maker(train_files, shape[1:])\n",
    "x_valid = keypoint_maker(valid_files, shape[1:])\n",
    "x_test = keypoint_maker(test_files, shape[1:])\n",
    "y_train = tf.keras.utils.to_categorical(train_targets, num_classes=number_of_classes)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_targets, num_classes=number_of_classes)\n",
    "y_test = tf.keras.utils.to_categorical(test_targets, num_classes=number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c9bb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Shape of all data:\", shape)\n",
    "print(\"Shape of training data:\", x_train.shape)\n",
    "print(\"Shape of training labels:\", y_train.shape)\n",
    "print(\"Shape of validation data:\", x_valid.shape)\n",
    "print(\"Shape of validation labels:\", y_valid.shape)\n",
    "print(\"Shape of testing data:\", x_test.shape)\n",
    "print(\"Shape of testing labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d38779",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model Layers & Paramers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab043b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "strides = 1\n",
    "padding= \"same\"\n",
    "activation_conv = \"tanh\"\n",
    "activation_lstm = \"tanh\"\n",
    "filters_conv = [128, 64, 32]\n",
    "filters_lstm = [128, 64, 32]\n",
    "kernel_size = [5,3,1]\n",
    "return_sequences = True\n",
    "\n",
    "MODEL = [\n",
    "    tf.keras.layers.Masking(mask_value = 0., input_shape = shape[1:]),\n",
    "    tf.keras.layers.Conv1D(filters_conv[0], kernel_size = kernel_size[0], strides = strides, padding = padding, activation = activation_conv),\n",
    "    tf.keras.layers.LSTM(filters_lstm[0], activation = activation_lstm, return_sequences = return_sequences),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(filters_conv[1], kernel_size = kernel_size[1], strides = strides, padding = padding, activation = activation_conv),\n",
    "    tf.keras.layers.LSTM(filters_lstm[1], activation = activation_lstm, return_sequences = return_sequences),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(filters_conv[2], kernel_size = kernel_size[2], strides = strides, padding = padding, activation = activation_conv),\n",
    "    tf.keras.layers.LSTM(filters_lstm[2], activation = activation_lstm, return_sequences = return_sequences),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(number_of_classes, activation=\"softmax\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d22b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(MODEL)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b443d25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,to_file = \"model.png\", show_shapes = True, show_layer_activations = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92343e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = x_train.shape[0]\n",
    "epochs = 2000\n",
    "learning_rate = 0.00001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [\"accuracy\"]\n",
    "model_file = \"model.hdf5\"\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = model_file, monitor = \"val_loss\", save_best_only = True)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history=model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_valid, y_valid),\n",
    "                  callbacks = callbacks, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dddbaa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955382ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_file)\n",
    "(loss, accuracy) = model.evaluate(x = x_test, y = y_test, batch_size = batch_size, verbose = 1)\n",
    "print(\"Accuracy on test data: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128186dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loss and Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c028a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af14047",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 16))\n",
    "plt.plot(epochs, acc, \"b\", label=  \"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"y\", label = \"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985d083",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 16))\n",
    "plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"y\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18396bdf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loss\")\n",
    "print(np.min(loss))\n",
    "print(np.argmin(loss))\n",
    "print(\"Validation Loss\")\n",
    "print(np.min(val_loss))\n",
    "print(np.argmin(val_loss))\n",
    "print(\"Accuracy\")\n",
    "print(np.max(acc))\n",
    "print(np.argmax(acc))\n",
    "print(\"Validation Accuracy\")\n",
    "print(np.max(val_acc))\n",
    "print(np.argmax(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50ca07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = list(test_targets)\n",
    "predictions = list(np.argmax(model.predict(x_test), axis=1))\n",
    "confusion_matrix = tf.math.confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1655b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Purples\", xticklabels = class_names, yticklabels = class_names).set(title = \"Confusion Matrix of Overall Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6289b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Rep Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539a849",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Rep_Counter(file_array, index, output_dir, log_output_filename):\n",
    "    log = open(output_dir + log_output_filename, \"a\", encoding=\"utf-8\")\n",
    "    print(\"#############################################################\", file=log)\n",
    "    filename = Path(file_array[index])\n",
    "\n",
    "    default_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    class_colours = [] \n",
    "    random.seed(seed)\n",
    "    for i in class_names:\n",
    "        B = random.randint(0, 255)\n",
    "        G = random.randint(0, 255)\n",
    "        R = random.randint(0, 255)\n",
    "        colour = tuple([B, G, R])\n",
    "        class_colours.append(colour)\n",
    "        \n",
    "    sequence = []\n",
    "    logger = [] #sequential log, gives exact order\n",
    "    predictions = []\n",
    "    \n",
    "    graph_data = [[] for i in range(0, number_of_classes)]\n",
    "    \n",
    "    threshold = 0.99\n",
    "    count = 1\n",
    "    cap = cv2.VideoCapture(file_array[index])\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_dir + filename.name, fourcc, fps, (width, height))\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=min_detect, min_tracking_confidence=min_track,\n",
    "                      model_complexity = model_complex, enable_segmentation=segmentation) as pose:\n",
    "        while cap.isOpened():\n",
    "            number_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            count += 1\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            exact_time_in_seconds = round(count/fps, 2)\n",
    "\n",
    "            image, results = mediapipe_detection(frame, pose)\n",
    "            keypoints = extract_keypoints(results)\n",
    "            \n",
    "            if keypoints is None:\n",
    "                continue\n",
    "            \n",
    "            sequence.append(keypoints)\n",
    "            sequence_np = np.array(sequence)\n",
    "            sequence_np = scaler.fit_transform(sequence_np)\n",
    "            sequence_np = pad_array(sequence_np, shape[1:])\n",
    "            \n",
    "            result = model.predict(np.expand_dims(sequence_np, axis=0))[0]\n",
    "            \n",
    "            for i in range(number_of_classes):\n",
    "                graph_data[i].append(result[i])\n",
    "            \n",
    "            predicted_class_index = np.argmax(result) #0, 1, 2                 \n",
    "            predicted_class_percentage = result[np.argmax(result)] #0.58%\n",
    "            predicted_class_percentage_rounded = round(predicted_class_percentage, 2)\n",
    "            predicted_class_name = class_names[np.argmax(result)] #catpass etc\n",
    "            predictions.append(predicted_class_index)\n",
    "\n",
    "            if np.unique(predictions[-10:])[0] == predicted_class_index: #checks last 10 frames have same value, might change\n",
    "                if predicted_class_percentage > threshold:   \n",
    "                    if len(logger) > 0:\n",
    "                        if predicted_class_name != logger[-1][0]:\n",
    "                            logger.append([predicted_class_name, exact_time_in_seconds, predicted_class_percentage_rounded])\n",
    "                    else:\n",
    "                        logger.append([predicted_class_name, exact_time_in_seconds, predicted_class_percentage_rounded])\n",
    "\n",
    "            #This does the probability boxes for each class\n",
    "            for class_index, probability in enumerate(result):\n",
    "                start_point = (0, 60 + class_index * 40)\n",
    "                end_point = (int(probability * 100), 90 + class_index * 40)\n",
    "                colour = class_colours[class_index]\n",
    "                thickness = -1\n",
    "                cv2.rectangle(image, start_point, end_point, colour, thickness)\n",
    "\n",
    "                text = \"{}:{}%\".format(class_names[class_index], int(probability*100))\n",
    "                org = (0, 85 + class_index * 40)\n",
    "                font_scale = 1\n",
    "                colour = (255,255,255)\n",
    "                thickness = 2\n",
    "                line_type = cv2.LINE_AA\n",
    "                cv2.putText(image, text, org, default_font, font_scale, colour, thickness, line_type)\n",
    "            \n",
    "\n",
    "            move_count_array = [0] * len(class_names)\n",
    "            if len(logger) > 0:\n",
    "                for log_value in logger: #pair e.g. catpass, 0.01 seconds, 98%\n",
    "                    index_to_increment = label_map.get(log_value[0])\n",
    "                    move_count_array[index_to_increment] += 1\n",
    "\n",
    "            text = \"{}\".format(logger)\n",
    "            org = (3, 30)\n",
    "            font_scale = 0.6\n",
    "            colour = (255,255,255)\n",
    "            thickness = 2\n",
    "            line_type = cv2.LINE_AA\n",
    "            cv2.putText(image, text, org, default_font, font_scale, colour, thickness, line_type)\n",
    "            cv2.imshow(\"Rep-Tracker\", image)\n",
    "            out.write(image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        count_array = [predictions.count(i) for i in range(len(class_names))]\n",
    "        \n",
    "        print(\"Video {}, {}\".format(index+1,filename.name), file=log)\n",
    "        print(\"###Repetiton Counter###\", file=log)\n",
    "        for i in range(len(class_names)):\n",
    "            print(\"{}={}\".format(class_names[i], move_count_array[i]), file=log)\n",
    "            \n",
    "        print(\"###Frames###\", file=log)\n",
    "        for i in range(len(class_names)):\n",
    "            print(\"{}={}\".format(class_names[i], count_array[i]), file=log)\n",
    "        \n",
    "        print(\"###Logger###\", file=log)\n",
    "        print(str(logger), file=log)\n",
    "            \n",
    "        print(\"Readable Frames: {}/{}\".format(len(graph_data[0]),number_of_frames), file=log)\n",
    "        print(\"Estimate: \\\"{}\\\" with {}/{} frames.\".format(class_names[np.argmax(count_array)], np.max(count_array),number_of_frames), file=log)\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    plt.figure(figsize=(20, 16))    \n",
    "    for i in range(number_of_classes):\n",
    "        plt.plot(graph_data[i], label=class_names[i])\n",
    "        \n",
    "    plt.title(\"Graph for {}\".format(filename.name))\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(output_dir+filename.stem)\n",
    "    return np.argmax(count_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_counter_accuracy(file_list, log_output_filename):\n",
    "    results = []\n",
    "    output = os.getcwd() + r\"/results/\"\n",
    "    \n",
    "    if not os.path.exists(output):\n",
    "        os.mkdir(output)\n",
    "    \n",
    "    for index in range(len(file_list)):\n",
    "        results.append(Rep_Counter(test_files, index, output, log_output_filename))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(labels, outputs):\n",
    "    count = 0\n",
    "    for index in range(len(labels)):\n",
    "        if labels[index] == outputs[index]:\n",
    "            count += 1\n",
    "    return (count/len(labels))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee5776",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Rep Counter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a87d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = list(test_targets)\n",
    "accuracy_array = rep_counter_accuracy(test_files, str(datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_percentage = similarity(labels, accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed974df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(accuracy_array)\n",
    "print(similarity_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6fd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(labels, accuracy_array)\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names).set(title=\"Confusion Matrix of Frame Prediction Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
