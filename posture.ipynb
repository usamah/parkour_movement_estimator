{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51df52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3760e771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True == False:\n",
    "    !pip install --upgrade pip\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install numba\n",
    "    !pip install sklearn\n",
    "    !pip install mediapipe\n",
    "    !pip install tqdm\n",
    "    !pip install seaborn\n",
    "    !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3572b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205f88ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Seed data, to ensure that between runs, the results would not differ\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44410a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e33d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to pad the data array with 0s\n",
    "def pad_array(input_array, new_shape):\n",
    "    first, second = np.shape(input_array)\n",
    "    output_array = np.zeros(new_shape)\n",
    "    output_array[0:first, 0:second] = input_array\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103dfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise function to normalise the keypoint data so it's easier for the network\n",
    "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(10, 11))\n",
    "\n",
    "#Numeric labeller\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8561e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mediapipe parameters and variables\n",
    "min_detect = 0.7 #non-default\n",
    "min_track = 0.7 #non-default\n",
    "segmentation = False #default\n",
    "model_complex = 1 #default\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20dde68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(image, pose):\n",
    "    # Convert image to RGB and process with pose\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = pose.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw keypoints and connections on the image\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "    # Extract keypoints if available\n",
    "    if results.pose_landmarks:\n",
    "        keypoints = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten()\n",
    "        return image, keypoints\n",
    "    else:\n",
    "        return image, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "101b3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(data_directory=os.getcwd() + \"/data/\", save_frames=False):\n",
    "    frame_array = []\n",
    "    fps_array = []\n",
    "    class_names = []\n",
    "    full_file_names = []\n",
    "\n",
    "    if save_frames:\n",
    "        frame_folder = \"/frames_\" + str(min_detect) + \"_\" + str(min_track) + \"_\" + str(segmentation) + \"_\" + str(model_complex) + \"/\"\n",
    "        frame_path = os.getcwd() + frame_folder\n",
    "        if not os.path.exists(frame_path):\n",
    "            os.mkdir(frame_path)\n",
    "\n",
    "    with mp.solutions.pose.Pose(min_detection_confidence=min_detect, min_tracking_confidence=min_track, model_complexity=model_complex, enable_segmentation=segmentation) as pose:\n",
    "        # Iterate through each class directory with a progress bar\n",
    "        for class_name in tqdm(os.listdir(data_directory), desc=\"Processing classes\"):\n",
    "            class_path = os.path.join(data_directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                # Process each video in class directory with a progress bar\n",
    "                for video_file in tqdm(os.listdir(class_path), desc=f\"Processing videos in {class_name}\"):\n",
    "                    video_path = os.path.join(class_path, video_file)\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    count = 0\n",
    "\n",
    "                    while cap.isOpened():\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "\n",
    "                        if save_frames:\n",
    "                            image, _ = extract_keypoints(frame, pose)\n",
    "                            image_file_name = os.path.join(frame_path, f\"{Path(video_file).stem}_{count}.jpeg\")\n",
    "                            cv2.imwrite(image_file_name, image)\n",
    "                        \n",
    "                        count += 1\n",
    "\n",
    "                    frame_array.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "                    fps_array.append(cap.get(cv2.CAP_PROP_FPS))\n",
    "                    class_names.append(class_name)\n",
    "                    full_file_names.append(video_path)  # Storing full file path\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "\n",
    "    return frame_array, fps_array, class_names, full_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c577af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_label_maker(video_list, shape):\n",
    "    video_array = []\n",
    "    label_array = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=min_detect, min_tracking_confidence=min_track,\n",
    "                      model_complexity=model_complex, enable_segmentation=segmentation) as pose:\n",
    "        \n",
    "        for video_path in tqdm(video_list):\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            keypoints_list = []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                _, keypoints = extract_keypoints(frame, pose)\n",
    "                if keypoints is not None:\n",
    "                    keypoints_list.append(keypoints)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if keypoints_list:  # Only proceed if keypoints are found\n",
    "                video = np.stack(keypoints_list, axis=0)\n",
    "                video = scaler.fit_transform(video)\n",
    "                video = pad_array(video, shape)  # Pads to largest video\n",
    "                video = np.expand_dims(video, axis=0)  # Turns (frame_size, keypoints_length) into (1, frame_size, keypoints_length)\n",
    "                video_array.append(video)  # Add current video to list\n",
    "                label = os.path.basename(os.path.dirname(video_path))  # Extract class name as label\n",
    "                numeric_label = label_encoder.transform([label])[0]\n",
    "                label_array.append(numeric_label)\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        tensor = np.vstack(video_array) if video_array else np.array([])\n",
    "        return tensor, np.array(label_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6a284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, keypoints, labels, transform=None):\n",
    "        self.keypoints = np.array(keypoints)\n",
    "        self.labels = np.array(labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keypoints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = torch.tensor(self.keypoints[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            keypoints = self.transform(keypoints)\n",
    "        return keypoints, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646f0f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705524357.369874  113095 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705524357.446376  113166 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.29.06), renderer: NVIDIA GeForce GTX 1080/PCIe/SSE2\n",
      "Processing classes:   0%|          | 0/3 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Processing videos in double_catpass: 100%|██████████| 60/60 [00:01<00:00, 35.69it/s]\n",
      "Processing videos in running_arm_jump: 100%|██████████| 60/60 [00:01<00:00, 38.25it/s]\n",
      "Processing videos in standing_plyo_stick: 100%|██████████| 60/60 [00:01<00:00, 45.06it/s]\n",
      "Processing classes: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "video_data = process_dataset(os.getcwd() + r\"/data\")\n",
    "files = video_data[3]\n",
    "all_classes = video_data[2]\n",
    "class_names = np.unique(video_data[2])\n",
    "number_of_classes = len(class_names)\n",
    "frame_limit = max(video_data[0])\n",
    "fps_array = video_data[1]\n",
    "fps_min = min(fps_array)\n",
    "fps_max = max(fps_array)\n",
    "shape = list((len(files), frame_limit, 132)) #number of videos, number of frames, number of keypoints\n",
    "label_map = {label:num for num, label in enumerate(class_names)}\n",
    "numeric_labels = label_encoder.fit_transform(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6580c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0705e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files, train_targets, test_targets = sklearn.model_selection.train_test_split(files, numeric_labels, test_size=1 - train_ratio, random_state = seed,  stratify = numeric_labels)\n",
    "valid_files, test_files, valid_targets, test_targets = sklearn.model_selection.train_test_split(test_files,test_targets, test_size = test_ratio / (test_ratio + validation_ratio), random_state = seed, stratify = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b09bed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1705524362.141561  113095 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705524362.212981  114616 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.29.06), renderer: NVIDIA GeForce GTX 1080/PCIe/SSE2\n",
      "100%|██████████| 144/144 [03:46<00:00,  1.57s/it]\n",
      "I0000 00:00:1705524588.783634  113095 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705524588.872091  116234 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.29.06), renderer: NVIDIA GeForce GTX 1080/PCIe/SSE2\n",
      "100%|██████████| 18/18 [00:22<00:00,  1.23s/it]\n",
      "I0000 00:00:1705524610.987365  113095 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705524611.089971  116393 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.29.06), renderer: NVIDIA GeForce GTX 1080/PCIe/SSE2\n",
      "100%|██████████| 18/18 [00:26<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = tensor_label_maker(train_files, shape[1:])\n",
    "X_val, y_val = tensor_label_maker(valid_files, shape[1:])\n",
    "X_test, y_test = tensor_label_maker(test_files, shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6859e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = KeypointDataset(X_train, y_train)\n",
    "val_dataset = KeypointDataset(X_val, y_val)\n",
    "test_dataset = KeypointDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc0a48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv1): Conv1d(132, 128, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (lstm1): LSTM(128, 128, batch_first=True)\n",
      "  (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (lstm2): LSTM(64, 64, batch_first=True)\n",
      "  (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv1d(64, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
      "  (lstm3): LSTM(32, 32, batch_first=True)\n",
      "  (norm3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Linear(in_features=6016, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, number_of_classes, shape):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.num_frames = shape[1]  # 51 frames\n",
    "        self.num_keypoints = shape[2]  # 132 keypoints\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=self.num_keypoints, out_channels=128, kernel_size=5, stride=1, padding='same')\n",
    "        self.lstm1 = torch.nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(num_features=128)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.lstm2 = torch.nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(num_features=64)\n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding='same')\n",
    "        self.lstm3 = torch.nn.LSTM(input_size=32, hidden_size=32, batch_first=True)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(num_features=32)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = torch.nn.Linear(32 * self.num_frames, number_of_classes)  # Adjust based on LSTM output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Adjusting the input for Conv1D: [batch, keypoints, frames]\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = x.permute(0, 2, 1)  # Adjusting for LSTM: [batch, frames, features]\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.permute(0, 2, 1)  # Adjusting for BatchNorm1D: [batch, features, frames]\n",
    "        x = self.norm1(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)  # Adjusting for LSTM\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x.permute(0, 2, 1)  # Adjusting for BatchNorm1D\n",
    "        x = self.norm2(x)\n",
    "        x = torch.tanh(self.conv3(x))\n",
    "        x = x.permute(0, 2, 1)  # Adjusting for LSTM\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = x.permute(0, 2, 1)  # Adjusting for BatchNorm1D\n",
    "        x = self.norm3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model with the number of classes and shape\n",
    "model = NeuralNetwork(number_of_classes, shape).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "649770d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation equivalent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "663c618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "298b584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(val_loader, model, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d5c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader, model, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0804fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_dataset.keypoints.shape[0]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "755e3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sammy/miniconda3/envs/posturepytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 1.1447, Training Accuracy: 29.86%\n",
      "Epoch 1 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 2/2000\n",
      "Epoch 2 Training Loss: 1.1285, Training Accuracy: 31.25%\n",
      "Epoch 2 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 3/2000\n",
      "Epoch 3 Training Loss: 1.1137, Training Accuracy: 33.33%\n",
      "Epoch 3 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 4/2000\n",
      "Epoch 4 Training Loss: 1.1012, Training Accuracy: 35.42%\n",
      "Epoch 4 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 5/2000\n",
      "Epoch 5 Training Loss: 1.0905, Training Accuracy: 46.53%\n",
      "Epoch 5 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 6/2000\n",
      "Epoch 6 Training Loss: 1.0815, Training Accuracy: 49.31%\n",
      "Epoch 6 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 7/2000\n",
      "Epoch 7 Training Loss: 1.0740, Training Accuracy: 50.69%\n",
      "Epoch 7 Validation Loss: 1.0987, Validation Accuracy: 33.33%\n",
      "Epoch 8/2000\n",
      "Epoch 8 Training Loss: 1.0675, Training Accuracy: 48.61%\n",
      "Epoch 8 Validation Loss: 1.0987, Validation Accuracy: 27.78%\n",
      "Epoch 9/2000\n",
      "Epoch 9 Training Loss: 1.0620, Training Accuracy: 49.31%\n",
      "Epoch 9 Validation Loss: 1.0987, Validation Accuracy: 38.89%\n",
      "Epoch 10/2000\n",
      "Epoch 10 Training Loss: 1.0577, Training Accuracy: 49.31%\n",
      "Epoch 10 Validation Loss: 1.0986, Validation Accuracy: 33.33%\n",
      "Epoch 11/2000\n",
      "Epoch 11 Training Loss: 1.0537, Training Accuracy: 49.31%\n",
      "Epoch 11 Validation Loss: 1.0986, Validation Accuracy: 33.33%\n",
      "Epoch 12/2000\n",
      "Epoch 12 Training Loss: 1.0491, Training Accuracy: 49.31%\n",
      "Epoch 12 Validation Loss: 1.0986, Validation Accuracy: 33.33%\n",
      "Epoch 13/2000\n",
      "Epoch 13 Training Loss: 1.0439, Training Accuracy: 49.31%\n",
      "Epoch 13 Validation Loss: 1.0986, Validation Accuracy: 33.33%\n",
      "Epoch 14/2000\n",
      "Epoch 14 Training Loss: 1.0391, Training Accuracy: 50.00%\n",
      "Epoch 14 Validation Loss: 1.0985, Validation Accuracy: 33.33%\n",
      "Epoch 15/2000\n",
      "Epoch 15 Training Loss: 1.0350, Training Accuracy: 49.31%\n",
      "Epoch 15 Validation Loss: 1.0984, Validation Accuracy: 33.33%\n",
      "Epoch 16/2000\n",
      "Epoch 16 Training Loss: 1.0311, Training Accuracy: 50.00%\n",
      "Epoch 16 Validation Loss: 1.0984, Validation Accuracy: 33.33%\n",
      "Epoch 17/2000\n",
      "Epoch 17 Training Loss: 1.0274, Training Accuracy: 50.00%\n",
      "Epoch 17 Validation Loss: 1.0983, Validation Accuracy: 33.33%\n",
      "Epoch 18/2000\n",
      "Epoch 18 Training Loss: 1.0245, Training Accuracy: 50.69%\n",
      "Epoch 18 Validation Loss: 1.0982, Validation Accuracy: 33.33%\n",
      "Epoch 19/2000\n",
      "Epoch 19 Training Loss: 1.0221, Training Accuracy: 49.31%\n",
      "Epoch 19 Validation Loss: 1.0981, Validation Accuracy: 33.33%\n",
      "Epoch 20/2000\n",
      "Epoch 20 Training Loss: 1.0193, Training Accuracy: 49.31%\n",
      "Epoch 20 Validation Loss: 1.0979, Validation Accuracy: 33.33%\n",
      "Epoch 21/2000\n",
      "Epoch 21 Training Loss: 1.0159, Training Accuracy: 48.61%\n",
      "Epoch 21 Validation Loss: 1.0977, Validation Accuracy: 33.33%\n",
      "Epoch 22/2000\n",
      "Epoch 22 Training Loss: 1.0124, Training Accuracy: 50.00%\n",
      "Epoch 22 Validation Loss: 1.0975, Validation Accuracy: 33.33%\n",
      "Epoch 23/2000\n",
      "Epoch 23 Training Loss: 1.0088, Training Accuracy: 51.39%\n",
      "Epoch 23 Validation Loss: 1.0973, Validation Accuracy: 33.33%\n",
      "Epoch 24/2000\n",
      "Epoch 24 Training Loss: 1.0051, Training Accuracy: 51.39%\n",
      "Epoch 24 Validation Loss: 1.0970, Validation Accuracy: 33.33%\n",
      "Epoch 25/2000\n",
      "Epoch 25 Training Loss: 1.0015, Training Accuracy: 54.17%\n",
      "Epoch 25 Validation Loss: 1.0966, Validation Accuracy: 33.33%\n",
      "Epoch 26/2000\n",
      "Epoch 26 Training Loss: 0.9982, Training Accuracy: 59.72%\n",
      "Epoch 26 Validation Loss: 1.0962, Validation Accuracy: 33.33%\n",
      "Epoch 27/2000\n",
      "Epoch 27 Training Loss: 0.9950, Training Accuracy: 62.50%\n",
      "Epoch 27 Validation Loss: 1.0957, Validation Accuracy: 33.33%\n",
      "Epoch 28/2000\n",
      "Epoch 28 Training Loss: 0.9919, Training Accuracy: 62.50%\n",
      "Epoch 28 Validation Loss: 1.0952, Validation Accuracy: 33.33%\n",
      "Epoch 29/2000\n",
      "Epoch 29 Training Loss: 0.9887, Training Accuracy: 60.42%\n",
      "Epoch 29 Validation Loss: 1.0946, Validation Accuracy: 33.33%\n",
      "Epoch 30/2000\n",
      "Epoch 30 Training Loss: 0.9856, Training Accuracy: 60.42%\n",
      "Epoch 30 Validation Loss: 1.0939, Validation Accuracy: 33.33%\n",
      "Epoch 31/2000\n",
      "Epoch 31 Training Loss: 0.9827, Training Accuracy: 61.81%\n",
      "Epoch 31 Validation Loss: 1.0931, Validation Accuracy: 33.33%\n",
      "Epoch 32/2000\n",
      "Epoch 32 Training Loss: 0.9797, Training Accuracy: 61.81%\n",
      "Epoch 32 Validation Loss: 1.0922, Validation Accuracy: 33.33%\n",
      "Epoch 33/2000\n",
      "Epoch 33 Training Loss: 0.9766, Training Accuracy: 62.50%\n",
      "Epoch 33 Validation Loss: 1.0911, Validation Accuracy: 33.33%\n",
      "Epoch 34/2000\n",
      "Epoch 34 Training Loss: 0.9736, Training Accuracy: 61.11%\n",
      "Epoch 34 Validation Loss: 1.0899, Validation Accuracy: 44.44%\n",
      "Epoch 35/2000\n",
      "Epoch 35 Training Loss: 0.9708, Training Accuracy: 62.50%\n",
      "Epoch 35 Validation Loss: 1.0886, Validation Accuracy: 61.11%\n",
      "Epoch 36/2000\n",
      "Epoch 36 Training Loss: 0.9678, Training Accuracy: 62.50%\n",
      "Epoch 36 Validation Loss: 1.0872, Validation Accuracy: 61.11%\n",
      "Epoch 37/2000\n",
      "Epoch 37 Training Loss: 0.9648, Training Accuracy: 63.19%\n",
      "Epoch 37 Validation Loss: 1.0855, Validation Accuracy: 66.67%\n",
      "Epoch 38/2000\n",
      "Epoch 38 Training Loss: 0.9620, Training Accuracy: 64.58%\n",
      "Epoch 38 Validation Loss: 1.0837, Validation Accuracy: 61.11%\n",
      "Epoch 39/2000\n",
      "Epoch 39 Training Loss: 0.9592, Training Accuracy: 64.58%\n",
      "Epoch 39 Validation Loss: 1.0817, Validation Accuracy: 55.56%\n",
      "Epoch 40/2000\n",
      "Epoch 40 Training Loss: 0.9563, Training Accuracy: 65.97%\n",
      "Epoch 40 Validation Loss: 1.0795, Validation Accuracy: 55.56%\n",
      "Epoch 41/2000\n",
      "Epoch 41 Training Loss: 0.9534, Training Accuracy: 66.67%\n",
      "Epoch 41 Validation Loss: 1.0771, Validation Accuracy: 66.67%\n",
      "Epoch 42/2000\n",
      "Epoch 42 Training Loss: 0.9506, Training Accuracy: 66.67%\n",
      "Epoch 42 Validation Loss: 1.0745, Validation Accuracy: 55.56%\n",
      "Epoch 43/2000\n",
      "Epoch 43 Training Loss: 0.9479, Training Accuracy: 66.67%\n",
      "Epoch 43 Validation Loss: 1.0718, Validation Accuracy: 55.56%\n",
      "Epoch 44/2000\n",
      "Epoch 44 Training Loss: 0.9452, Training Accuracy: 66.67%\n",
      "Epoch 44 Validation Loss: 1.0690, Validation Accuracy: 55.56%\n",
      "Epoch 45/2000\n",
      "Epoch 45 Training Loss: 0.9425, Training Accuracy: 67.36%\n",
      "Epoch 45 Validation Loss: 1.0660, Validation Accuracy: 55.56%\n",
      "Epoch 46/2000\n",
      "Epoch 46 Training Loss: 0.9400, Training Accuracy: 67.36%\n",
      "Epoch 46 Validation Loss: 1.0630, Validation Accuracy: 55.56%\n",
      "Epoch 47/2000\n",
      "Epoch 47 Training Loss: 0.9374, Training Accuracy: 66.67%\n",
      "Epoch 47 Validation Loss: 1.0598, Validation Accuracy: 55.56%\n",
      "Epoch 48/2000\n",
      "Epoch 48 Training Loss: 0.9348, Training Accuracy: 66.67%\n",
      "Epoch 48 Validation Loss: 1.0565, Validation Accuracy: 55.56%\n",
      "Epoch 49/2000\n",
      "Epoch 49 Training Loss: 0.9320, Training Accuracy: 66.67%\n",
      "Epoch 49 Validation Loss: 1.0530, Validation Accuracy: 55.56%\n",
      "Epoch 50/2000\n",
      "Epoch 50 Training Loss: 0.9293, Training Accuracy: 67.36%\n",
      "Epoch 50 Validation Loss: 1.0495, Validation Accuracy: 55.56%\n",
      "Epoch 51/2000\n",
      "Epoch 51 Training Loss: 0.9268, Training Accuracy: 66.67%\n",
      "Epoch 51 Validation Loss: 1.0457, Validation Accuracy: 55.56%\n",
      "Epoch 52/2000\n",
      "Epoch 52 Training Loss: 0.9244, Training Accuracy: 66.67%\n",
      "Epoch 52 Validation Loss: 1.0417, Validation Accuracy: 55.56%\n",
      "Epoch 53/2000\n",
      "Epoch 53 Training Loss: 0.9219, Training Accuracy: 66.67%\n",
      "Epoch 53 Validation Loss: 1.0376, Validation Accuracy: 55.56%\n",
      "Epoch 54/2000\n",
      "Epoch 54 Training Loss: 0.9193, Training Accuracy: 67.36%\n",
      "Epoch 54 Validation Loss: 1.0334, Validation Accuracy: 55.56%\n",
      "Epoch 55/2000\n",
      "Epoch 55 Training Loss: 0.9166, Training Accuracy: 68.06%\n",
      "Epoch 55 Validation Loss: 1.0293, Validation Accuracy: 55.56%\n",
      "Epoch 56/2000\n",
      "Epoch 56 Training Loss: 0.9140, Training Accuracy: 68.75%\n",
      "Epoch 56 Validation Loss: 1.0250, Validation Accuracy: 55.56%\n",
      "Epoch 57/2000\n",
      "Epoch 57 Training Loss: 0.9114, Training Accuracy: 68.75%\n",
      "Epoch 57 Validation Loss: 1.0205, Validation Accuracy: 55.56%\n",
      "Epoch 58/2000\n",
      "Epoch 58 Training Loss: 0.9088, Training Accuracy: 68.75%\n",
      "Epoch 58 Validation Loss: 1.0160, Validation Accuracy: 55.56%\n",
      "Epoch 59/2000\n",
      "Epoch 59 Training Loss: 0.9063, Training Accuracy: 68.75%\n",
      "Epoch 59 Validation Loss: 1.0116, Validation Accuracy: 55.56%\n",
      "Epoch 60/2000\n",
      "Epoch 60 Training Loss: 0.9038, Training Accuracy: 68.75%\n",
      "Epoch 60 Validation Loss: 1.0071, Validation Accuracy: 55.56%\n",
      "Epoch 61/2000\n",
      "Epoch 61 Training Loss: 0.9012, Training Accuracy: 69.44%\n",
      "Epoch 61 Validation Loss: 1.0025, Validation Accuracy: 55.56%\n",
      "Epoch 62/2000\n",
      "Epoch 62 Training Loss: 0.8985, Training Accuracy: 69.44%\n",
      "Epoch 62 Validation Loss: 0.9975, Validation Accuracy: 55.56%\n",
      "Epoch 63/2000\n",
      "Epoch 63 Training Loss: 0.8958, Training Accuracy: 70.83%\n",
      "Epoch 63 Validation Loss: 0.9923, Validation Accuracy: 55.56%\n",
      "Epoch 64/2000\n",
      "Epoch 64 Training Loss: 0.8932, Training Accuracy: 71.53%\n",
      "Epoch 64 Validation Loss: 0.9871, Validation Accuracy: 55.56%\n",
      "Epoch 65/2000\n",
      "Epoch 65 Training Loss: 0.8905, Training Accuracy: 73.61%\n",
      "Epoch 65 Validation Loss: 0.9822, Validation Accuracy: 55.56%\n",
      "Epoch 66/2000\n",
      "Epoch 66 Training Loss: 0.8878, Training Accuracy: 73.61%\n",
      "Epoch 66 Validation Loss: 0.9775, Validation Accuracy: 55.56%\n",
      "Epoch 67/2000\n",
      "Epoch 67 Training Loss: 0.8850, Training Accuracy: 73.61%\n",
      "Epoch 67 Validation Loss: 0.9729, Validation Accuracy: 55.56%\n",
      "Epoch 68/2000\n",
      "Epoch 68 Training Loss: 0.8823, Training Accuracy: 74.31%\n",
      "Epoch 68 Validation Loss: 0.9684, Validation Accuracy: 55.56%\n",
      "Epoch 69/2000\n",
      "Epoch 69 Training Loss: 0.8795, Training Accuracy: 74.31%\n",
      "Epoch 69 Validation Loss: 0.9644, Validation Accuracy: 61.11%\n",
      "Epoch 70/2000\n",
      "Epoch 70 Training Loss: 0.8769, Training Accuracy: 74.31%\n",
      "Epoch 70 Validation Loss: 0.9602, Validation Accuracy: 61.11%\n",
      "Epoch 71/2000\n",
      "Epoch 71 Training Loss: 0.8742, Training Accuracy: 74.31%\n",
      "Epoch 71 Validation Loss: 0.9575, Validation Accuracy: 61.11%\n",
      "Epoch 72/2000\n",
      "Epoch 72 Training Loss: 0.8716, Training Accuracy: 74.31%\n",
      "Epoch 72 Validation Loss: 0.9522, Validation Accuracy: 61.11%\n",
      "Epoch 73/2000\n",
      "Epoch 73 Training Loss: 0.8690, Training Accuracy: 75.69%\n",
      "Epoch 73 Validation Loss: 0.9516, Validation Accuracy: 61.11%\n",
      "Epoch 74/2000\n",
      "Epoch 74 Training Loss: 0.8665, Training Accuracy: 75.69%\n",
      "Epoch 74 Validation Loss: 0.9443, Validation Accuracy: 61.11%\n",
      "Epoch 75/2000\n",
      "Epoch 75 Training Loss: 0.8639, Training Accuracy: 76.39%\n",
      "Epoch 75 Validation Loss: 0.9427, Validation Accuracy: 61.11%\n",
      "Epoch 76/2000\n",
      "Epoch 76 Training Loss: 0.8612, Training Accuracy: 76.39%\n",
      "Epoch 76 Validation Loss: 0.9393, Validation Accuracy: 61.11%\n",
      "Epoch 77/2000\n",
      "Epoch 77 Training Loss: 0.8587, Training Accuracy: 76.39%\n",
      "Epoch 77 Validation Loss: 0.9313, Validation Accuracy: 61.11%\n",
      "Epoch 78/2000\n",
      "Epoch 78 Training Loss: 0.8563, Training Accuracy: 76.39%\n",
      "Epoch 78 Validation Loss: 0.9307, Validation Accuracy: 61.11%\n",
      "Epoch 79/2000\n",
      "Epoch 79 Training Loss: 0.8537, Training Accuracy: 76.39%\n",
      "Epoch 79 Validation Loss: 0.9246, Validation Accuracy: 61.11%\n",
      "Epoch 80/2000\n",
      "Epoch 80 Training Loss: 0.8511, Training Accuracy: 76.39%\n",
      "Epoch 80 Validation Loss: 0.9194, Validation Accuracy: 61.11%\n",
      "Epoch 81/2000\n",
      "Epoch 81 Training Loss: 0.8487, Training Accuracy: 77.08%\n",
      "Epoch 81 Validation Loss: 0.9196, Validation Accuracy: 66.67%\n",
      "Epoch 82/2000\n",
      "Epoch 82 Training Loss: 0.8464, Training Accuracy: 77.08%\n",
      "Epoch 82 Validation Loss: 0.9114, Validation Accuracy: 66.67%\n",
      "Epoch 83/2000\n",
      "Epoch 83 Training Loss: 0.8440, Training Accuracy: 77.78%\n",
      "Epoch 83 Validation Loss: 0.9116, Validation Accuracy: 66.67%\n",
      "Epoch 84/2000\n",
      "Epoch 84 Training Loss: 0.8415, Training Accuracy: 77.78%\n",
      "Epoch 84 Validation Loss: 0.9064, Validation Accuracy: 72.22%\n",
      "Epoch 85/2000\n",
      "Epoch 85 Training Loss: 0.8391, Training Accuracy: 77.78%\n",
      "Epoch 85 Validation Loss: 0.9028, Validation Accuracy: 72.22%\n",
      "Epoch 86/2000\n",
      "Epoch 86 Training Loss: 0.8367, Training Accuracy: 77.08%\n",
      "Epoch 86 Validation Loss: 0.9027, Validation Accuracy: 72.22%\n",
      "Epoch 87/2000\n",
      "Epoch 87 Training Loss: 0.8345, Training Accuracy: 78.47%\n",
      "Epoch 87 Validation Loss: 0.8950, Validation Accuracy: 72.22%\n",
      "Epoch 88/2000\n",
      "Epoch 88 Training Loss: 0.8324, Training Accuracy: 77.78%\n",
      "Epoch 88 Validation Loss: 0.9005, Validation Accuracy: 72.22%\n",
      "Epoch 89/2000\n",
      "Epoch 89 Training Loss: 0.8305, Training Accuracy: 78.47%\n",
      "Epoch 89 Validation Loss: 0.8884, Validation Accuracy: 72.22%\n",
      "Epoch 90/2000\n",
      "Epoch 90 Training Loss: 0.8283, Training Accuracy: 77.78%\n",
      "Epoch 90 Validation Loss: 0.8933, Validation Accuracy: 72.22%\n",
      "Epoch 91/2000\n",
      "Epoch 91 Training Loss: 0.8254, Training Accuracy: 78.47%\n",
      "Epoch 91 Validation Loss: 0.8878, Validation Accuracy: 72.22%\n",
      "Epoch 92/2000\n",
      "Epoch 92 Training Loss: 0.8228, Training Accuracy: 77.78%\n",
      "Epoch 92 Validation Loss: 0.8815, Validation Accuracy: 72.22%\n",
      "Epoch 93/2000\n",
      "Epoch 93 Training Loss: 0.8210, Training Accuracy: 78.47%\n",
      "Epoch 93 Validation Loss: 0.8873, Validation Accuracy: 72.22%\n",
      "Epoch 94/2000\n",
      "Epoch 94 Training Loss: 0.8190, Training Accuracy: 79.17%\n",
      "Epoch 94 Validation Loss: 0.8774, Validation Accuracy: 72.22%\n",
      "Epoch 95/2000\n",
      "Epoch 95 Training Loss: 0.8164, Training Accuracy: 78.47%\n",
      "Epoch 95 Validation Loss: 0.8773, Validation Accuracy: 72.22%\n",
      "Epoch 96/2000\n",
      "Epoch 96 Training Loss: 0.8140, Training Accuracy: 77.78%\n",
      "Epoch 96 Validation Loss: 0.8797, Validation Accuracy: 72.22%\n",
      "Epoch 97/2000\n",
      "Epoch 97 Training Loss: 0.8122, Training Accuracy: 78.47%\n",
      "Epoch 97 Validation Loss: 0.8708, Validation Accuracy: 72.22%\n",
      "Epoch 98/2000\n",
      "Epoch 98 Training Loss: 0.8105, Training Accuracy: 78.47%\n",
      "Epoch 98 Validation Loss: 0.8771, Validation Accuracy: 72.22%\n",
      "Epoch 99/2000\n",
      "Epoch 99 Training Loss: 0.8081, Training Accuracy: 79.17%\n",
      "Epoch 99 Validation Loss: 0.8700, Validation Accuracy: 72.22%\n",
      "Epoch 100/2000\n",
      "Epoch 100 Training Loss: 0.8056, Training Accuracy: 79.17%\n",
      "Epoch 100 Validation Loss: 0.8668, Validation Accuracy: 72.22%\n",
      "Epoch 101/2000\n",
      "Epoch 101 Training Loss: 0.8036, Training Accuracy: 79.86%\n",
      "Epoch 101 Validation Loss: 0.8716, Validation Accuracy: 72.22%\n",
      "Epoch 102/2000\n",
      "Epoch 102 Training Loss: 0.8021, Training Accuracy: 80.56%\n",
      "Epoch 102 Validation Loss: 0.8606, Validation Accuracy: 72.22%\n",
      "Epoch 103/2000\n",
      "Epoch 103 Training Loss: 0.8000, Training Accuracy: 80.56%\n",
      "Epoch 103 Validation Loss: 0.8641, Validation Accuracy: 72.22%\n",
      "Epoch 104/2000\n",
      "Epoch 104 Training Loss: 0.7975, Training Accuracy: 79.86%\n",
      "Epoch 104 Validation Loss: 0.8591, Validation Accuracy: 72.22%\n",
      "Epoch 105/2000\n",
      "Epoch 105 Training Loss: 0.7952, Training Accuracy: 80.56%\n",
      "Epoch 105 Validation Loss: 0.8564, Validation Accuracy: 72.22%\n",
      "Epoch 106/2000\n",
      "Epoch 106 Training Loss: 0.7931, Training Accuracy: 80.56%\n",
      "Epoch 106 Validation Loss: 0.8585, Validation Accuracy: 72.22%\n",
      "Epoch 107/2000\n",
      "Epoch 107 Training Loss: 0.7915, Training Accuracy: 80.56%\n",
      "Epoch 107 Validation Loss: 0.8495, Validation Accuracy: 72.22%\n",
      "Epoch 108/2000\n",
      "Epoch 108 Training Loss: 0.7899, Training Accuracy: 82.64%\n",
      "Epoch 108 Validation Loss: 0.8545, Validation Accuracy: 72.22%\n",
      "Epoch 109/2000\n",
      "Epoch 109 Training Loss: 0.7877, Training Accuracy: 81.94%\n",
      "Epoch 109 Validation Loss: 0.8469, Validation Accuracy: 77.78%\n",
      "Epoch 110/2000\n",
      "Epoch 110 Training Loss: 0.7854, Training Accuracy: 84.03%\n",
      "Epoch 110 Validation Loss: 0.8467, Validation Accuracy: 72.22%\n",
      "Epoch 111/2000\n",
      "Epoch 111 Training Loss: 0.7830, Training Accuracy: 82.64%\n",
      "Epoch 111 Validation Loss: 0.8453, Validation Accuracy: 72.22%\n",
      "Epoch 112/2000\n",
      "Epoch 112 Training Loss: 0.7811, Training Accuracy: 83.33%\n",
      "Epoch 112 Validation Loss: 0.8415, Validation Accuracy: 77.78%\n",
      "Epoch 113/2000\n",
      "Epoch 113 Training Loss: 0.7795, Training Accuracy: 84.72%\n",
      "Epoch 113 Validation Loss: 0.8445, Validation Accuracy: 72.22%\n",
      "Epoch 114/2000\n",
      "Epoch 114 Training Loss: 0.7779, Training Accuracy: 84.03%\n",
      "Epoch 114 Validation Loss: 0.8352, Validation Accuracy: 77.78%\n",
      "Epoch 115/2000\n",
      "Epoch 115 Training Loss: 0.7762, Training Accuracy: 86.11%\n",
      "Epoch 115 Validation Loss: 0.8401, Validation Accuracy: 77.78%\n",
      "Epoch 116/2000\n",
      "Epoch 116 Training Loss: 0.7739, Training Accuracy: 84.03%\n",
      "Epoch 116 Validation Loss: 0.8329, Validation Accuracy: 77.78%\n",
      "Epoch 117/2000\n",
      "Epoch 117 Training Loss: 0.7714, Training Accuracy: 86.81%\n",
      "Epoch 117 Validation Loss: 0.8312, Validation Accuracy: 77.78%\n",
      "Epoch 118/2000\n",
      "Epoch 118 Training Loss: 0.7695, Training Accuracy: 86.81%\n",
      "Epoch 118 Validation Loss: 0.8324, Validation Accuracy: 77.78%\n",
      "Epoch 119/2000\n",
      "Epoch 119 Training Loss: 0.7680, Training Accuracy: 86.81%\n",
      "Epoch 119 Validation Loss: 0.8255, Validation Accuracy: 77.78%\n",
      "Epoch 120/2000\n",
      "Epoch 120 Training Loss: 0.7666, Training Accuracy: 88.19%\n",
      "Epoch 120 Validation Loss: 0.8313, Validation Accuracy: 77.78%\n",
      "Epoch 121/2000\n",
      "Epoch 121 Training Loss: 0.7650, Training Accuracy: 87.50%\n",
      "Epoch 121 Validation Loss: 0.8229, Validation Accuracy: 77.78%\n",
      "Epoch 122/2000\n",
      "Epoch 122 Training Loss: 0.7628, Training Accuracy: 89.58%\n",
      "Epoch 122 Validation Loss: 0.8238, Validation Accuracy: 77.78%\n",
      "Epoch 123/2000\n",
      "Epoch 123 Training Loss: 0.7605, Training Accuracy: 86.81%\n",
      "Epoch 123 Validation Loss: 0.8225, Validation Accuracy: 77.78%\n",
      "Epoch 124/2000\n",
      "Epoch 124 Training Loss: 0.7588, Training Accuracy: 86.81%\n",
      "Epoch 124 Validation Loss: 0.8172, Validation Accuracy: 77.78%\n",
      "Epoch 125/2000\n",
      "Epoch 125 Training Loss: 0.7576, Training Accuracy: 89.58%\n",
      "Epoch 125 Validation Loss: 0.8238, Validation Accuracy: 77.78%\n",
      "Epoch 126/2000\n",
      "Epoch 126 Training Loss: 0.7560, Training Accuracy: 88.19%\n",
      "Epoch 126 Validation Loss: 0.8139, Validation Accuracy: 77.78%\n",
      "Epoch 127/2000\n",
      "Epoch 127 Training Loss: 0.7541, Training Accuracy: 89.58%\n",
      "Epoch 127 Validation Loss: 0.8182, Validation Accuracy: 77.78%\n",
      "Epoch 128/2000\n",
      "Epoch 128 Training Loss: 0.7519, Training Accuracy: 88.19%\n",
      "Epoch 128 Validation Loss: 0.8158, Validation Accuracy: 77.78%\n",
      "Epoch 129/2000\n",
      "Epoch 129 Training Loss: 0.7500, Training Accuracy: 88.89%\n",
      "Epoch 129 Validation Loss: 0.8103, Validation Accuracy: 77.78%\n",
      "Epoch 130/2000\n",
      "Epoch 130 Training Loss: 0.7486, Training Accuracy: 89.58%\n",
      "Epoch 130 Validation Loss: 0.8167, Validation Accuracy: 77.78%\n",
      "Epoch 131/2000\n",
      "Epoch 131 Training Loss: 0.7472, Training Accuracy: 88.19%\n",
      "Epoch 131 Validation Loss: 0.8071, Validation Accuracy: 83.33%\n",
      "Epoch 132/2000\n",
      "Epoch 132 Training Loss: 0.7455, Training Accuracy: 90.28%\n",
      "Epoch 132 Validation Loss: 0.8140, Validation Accuracy: 77.78%\n",
      "Epoch 133/2000\n",
      "Epoch 133 Training Loss: 0.7435, Training Accuracy: 90.28%\n",
      "Epoch 133 Validation Loss: 0.8087, Validation Accuracy: 77.78%\n",
      "Epoch 134/2000\n",
      "Epoch 134 Training Loss: 0.7417, Training Accuracy: 90.28%\n",
      "Epoch 134 Validation Loss: 0.8070, Validation Accuracy: 77.78%\n",
      "Epoch 135/2000\n",
      "Epoch 135 Training Loss: 0.7400, Training Accuracy: 90.28%\n",
      "Epoch 135 Validation Loss: 0.8094, Validation Accuracy: 77.78%\n",
      "Epoch 136/2000\n",
      "Epoch 136 Training Loss: 0.7384, Training Accuracy: 89.58%\n",
      "Epoch 136 Validation Loss: 0.8020, Validation Accuracy: 83.33%\n",
      "Epoch 137/2000\n",
      "Epoch 137 Training Loss: 0.7369, Training Accuracy: 90.28%\n",
      "Epoch 137 Validation Loss: 0.8091, Validation Accuracy: 72.22%\n",
      "Epoch 138/2000\n",
      "Epoch 138 Training Loss: 0.7356, Training Accuracy: 90.28%\n",
      "Epoch 138 Validation Loss: 0.7998, Validation Accuracy: 83.33%\n",
      "Epoch 139/2000\n",
      "Epoch 139 Training Loss: 0.7344, Training Accuracy: 90.28%\n",
      "Epoch 139 Validation Loss: 0.8093, Validation Accuracy: 72.22%\n",
      "Epoch 140/2000\n",
      "Epoch 140 Training Loss: 0.7326, Training Accuracy: 90.97%\n",
      "Epoch 140 Validation Loss: 0.7976, Validation Accuracy: 83.33%\n",
      "Epoch 141/2000\n",
      "Epoch 141 Training Loss: 0.7307, Training Accuracy: 90.28%\n",
      "Epoch 141 Validation Loss: 0.8020, Validation Accuracy: 83.33%\n",
      "Epoch 142/2000\n",
      "Epoch 142 Training Loss: 0.7289, Training Accuracy: 91.67%\n",
      "Epoch 142 Validation Loss: 0.7972, Validation Accuracy: 88.89%\n",
      "Epoch 143/2000\n",
      "Epoch 143 Training Loss: 0.7273, Training Accuracy: 90.28%\n",
      "Epoch 143 Validation Loss: 0.7944, Validation Accuracy: 88.89%\n",
      "Epoch 144/2000\n",
      "Epoch 144 Training Loss: 0.7257, Training Accuracy: 90.28%\n",
      "Epoch 144 Validation Loss: 0.8014, Validation Accuracy: 83.33%\n",
      "Epoch 145/2000\n",
      "Epoch 145 Training Loss: 0.7246, Training Accuracy: 90.97%\n",
      "Epoch 145 Validation Loss: 0.7900, Validation Accuracy: 88.89%\n",
      "Epoch 146/2000\n",
      "Epoch 146 Training Loss: 0.7236, Training Accuracy: 90.28%\n",
      "Epoch 146 Validation Loss: 0.8041, Validation Accuracy: 77.78%\n",
      "Epoch 147/2000\n",
      "Epoch 147 Training Loss: 0.7222, Training Accuracy: 90.97%\n",
      "Epoch 147 Validation Loss: 0.7894, Validation Accuracy: 88.89%\n",
      "Epoch 148/2000\n",
      "Epoch 148 Training Loss: 0.7207, Training Accuracy: 90.28%\n",
      "Epoch 148 Validation Loss: 0.7967, Validation Accuracy: 88.89%\n",
      "Epoch 149/2000\n",
      "Epoch 149 Training Loss: 0.7191, Training Accuracy: 90.97%\n",
      "Epoch 149 Validation Loss: 0.7877, Validation Accuracy: 88.89%\n",
      "Epoch 150/2000\n",
      "Epoch 150 Training Loss: 0.7171, Training Accuracy: 90.97%\n",
      "Epoch 150 Validation Loss: 0.7901, Validation Accuracy: 88.89%\n",
      "Epoch 151/2000\n",
      "Epoch 151 Training Loss: 0.7154, Training Accuracy: 91.67%\n",
      "Epoch 151 Validation Loss: 0.7901, Validation Accuracy: 88.89%\n",
      "Epoch 152/2000\n",
      "Epoch 152 Training Loss: 0.7142, Training Accuracy: 91.67%\n",
      "Epoch 152 Validation Loss: 0.7841, Validation Accuracy: 88.89%\n",
      "Epoch 153/2000\n",
      "Epoch 153 Training Loss: 0.7131, Training Accuracy: 90.97%\n",
      "Epoch 153 Validation Loss: 0.7942, Validation Accuracy: 88.89%\n",
      "Epoch 154/2000\n",
      "Epoch 154 Training Loss: 0.7120, Training Accuracy: 91.67%\n",
      "Epoch 154 Validation Loss: 0.7781, Validation Accuracy: 88.89%\n",
      "Epoch 155/2000\n",
      "Epoch 155 Training Loss: 0.7109, Training Accuracy: 90.28%\n",
      "Epoch 155 Validation Loss: 0.7911, Validation Accuracy: 88.89%\n",
      "Epoch 156/2000\n",
      "Epoch 156 Training Loss: 0.7094, Training Accuracy: 90.97%\n",
      "Epoch 156 Validation Loss: 0.7791, Validation Accuracy: 88.89%\n",
      "Epoch 157/2000\n",
      "Epoch 157 Training Loss: 0.7078, Training Accuracy: 90.97%\n",
      "Epoch 157 Validation Loss: 0.7824, Validation Accuracy: 88.89%\n",
      "Epoch 158/2000\n",
      "Epoch 158 Training Loss: 0.7062, Training Accuracy: 91.67%\n",
      "Epoch 158 Validation Loss: 0.7801, Validation Accuracy: 88.89%\n",
      "Epoch 159/2000\n",
      "Epoch 159 Training Loss: 0.7046, Training Accuracy: 91.67%\n",
      "Epoch 159 Validation Loss: 0.7772, Validation Accuracy: 88.89%\n",
      "Epoch 160/2000\n",
      "Epoch 160 Training Loss: 0.7034, Training Accuracy: 91.67%\n",
      "Epoch 160 Validation Loss: 0.7820, Validation Accuracy: 88.89%\n",
      "Epoch 161/2000\n",
      "Epoch 161 Training Loss: 0.7024, Training Accuracy: 92.36%\n",
      "Epoch 161 Validation Loss: 0.7730, Validation Accuracy: 88.89%\n",
      "Epoch 162/2000\n",
      "Epoch 162 Training Loss: 0.7014, Training Accuracy: 91.67%\n",
      "Epoch 162 Validation Loss: 0.7851, Validation Accuracy: 88.89%\n",
      "Epoch 163/2000\n",
      "Epoch 163 Training Loss: 0.7004, Training Accuracy: 92.36%\n",
      "Epoch 163 Validation Loss: 0.7681, Validation Accuracy: 88.89%\n",
      "Epoch 164/2000\n",
      "Epoch 164 Training Loss: 0.6992, Training Accuracy: 92.36%\n",
      "Epoch 164 Validation Loss: 0.7809, Validation Accuracy: 88.89%\n",
      "Epoch 165/2000\n",
      "Epoch 165 Training Loss: 0.6977, Training Accuracy: 92.36%\n",
      "Epoch 165 Validation Loss: 0.7689, Validation Accuracy: 88.89%\n",
      "Epoch 166/2000\n",
      "Epoch 166 Training Loss: 0.6961, Training Accuracy: 93.06%\n",
      "Epoch 166 Validation Loss: 0.7705, Validation Accuracy: 88.89%\n",
      "Epoch 167/2000\n",
      "Epoch 167 Training Loss: 0.6946, Training Accuracy: 93.06%\n",
      "Epoch 167 Validation Loss: 0.7729, Validation Accuracy: 88.89%\n",
      "Epoch 168/2000\n",
      "Epoch 168 Training Loss: 0.6935, Training Accuracy: 93.06%\n",
      "Epoch 168 Validation Loss: 0.7653, Validation Accuracy: 88.89%\n",
      "Epoch 169/2000\n",
      "Epoch 169 Training Loss: 0.6927, Training Accuracy: 93.75%\n",
      "Epoch 169 Validation Loss: 0.7773, Validation Accuracy: 88.89%\n",
      "Epoch 170/2000\n",
      "Epoch 170 Training Loss: 0.6917, Training Accuracy: 93.06%\n",
      "Epoch 170 Validation Loss: 0.7634, Validation Accuracy: 88.89%\n",
      "Epoch 171/2000\n",
      "Epoch 171 Training Loss: 0.6903, Training Accuracy: 93.75%\n",
      "Epoch 171 Validation Loss: 0.7736, Validation Accuracy: 88.89%\n",
      "Epoch 172/2000\n",
      "Epoch 172 Training Loss: 0.6889, Training Accuracy: 93.75%\n",
      "Epoch 172 Validation Loss: 0.7659, Validation Accuracy: 88.89%\n",
      "Epoch 173/2000\n",
      "Epoch 173 Training Loss: 0.6876, Training Accuracy: 93.75%\n",
      "Epoch 173 Validation Loss: 0.7647, Validation Accuracy: 88.89%\n",
      "Epoch 174/2000\n",
      "Epoch 174 Training Loss: 0.6864, Training Accuracy: 93.75%\n",
      "Epoch 174 Validation Loss: 0.7687, Validation Accuracy: 88.89%\n",
      "Epoch 175/2000\n",
      "Epoch 175 Training Loss: 0.6852, Training Accuracy: 93.75%\n",
      "Epoch 175 Validation Loss: 0.7619, Validation Accuracy: 88.89%\n",
      "Epoch 176/2000\n",
      "Epoch 176 Training Loss: 0.6840, Training Accuracy: 93.75%\n",
      "Epoch 176 Validation Loss: 0.7682, Validation Accuracy: 88.89%\n",
      "Epoch 177/2000\n",
      "Epoch 177 Training Loss: 0.6830, Training Accuracy: 93.75%\n",
      "Epoch 177 Validation Loss: 0.7630, Validation Accuracy: 88.89%\n",
      "Epoch 178/2000\n",
      "Epoch 178 Training Loss: 0.6821, Training Accuracy: 93.75%\n",
      "Epoch 178 Validation Loss: 0.7697, Validation Accuracy: 88.89%\n",
      "Epoch 179/2000\n",
      "Epoch 179 Training Loss: 0.6810, Training Accuracy: 93.75%\n",
      "Epoch 179 Validation Loss: 0.7601, Validation Accuracy: 88.89%\n",
      "Epoch 180/2000\n",
      "Epoch 180 Training Loss: 0.6797, Training Accuracy: 93.75%\n",
      "Epoch 180 Validation Loss: 0.7699, Validation Accuracy: 88.89%\n",
      "Epoch 181/2000\n",
      "Epoch 181 Training Loss: 0.6785, Training Accuracy: 93.75%\n",
      "Epoch 181 Validation Loss: 0.7581, Validation Accuracy: 88.89%\n",
      "Epoch 182/2000\n",
      "Epoch 182 Training Loss: 0.6772, Training Accuracy: 93.75%\n",
      "Epoch 182 Validation Loss: 0.7648, Validation Accuracy: 88.89%\n",
      "Epoch 183/2000\n",
      "Epoch 183 Training Loss: 0.6759, Training Accuracy: 93.75%\n",
      "Epoch 183 Validation Loss: 0.7616, Validation Accuracy: 88.89%\n",
      "Epoch 184/2000\n",
      "Epoch 184 Training Loss: 0.6747, Training Accuracy: 94.44%\n",
      "Epoch 184 Validation Loss: 0.7586, Validation Accuracy: 88.89%\n",
      "Epoch 185/2000\n",
      "Epoch 185 Training Loss: 0.6736, Training Accuracy: 94.44%\n",
      "Epoch 185 Validation Loss: 0.7677, Validation Accuracy: 88.89%\n",
      "Epoch 186/2000\n",
      "Epoch 186 Training Loss: 0.6727, Training Accuracy: 94.44%\n",
      "Epoch 186 Validation Loss: 0.7556, Validation Accuracy: 88.89%\n",
      "Epoch 187/2000\n",
      "Epoch 187 Training Loss: 0.6718, Training Accuracy: 95.14%\n",
      "Epoch 187 Validation Loss: 0.7699, Validation Accuracy: 83.33%\n",
      "Epoch 188/2000\n",
      "Epoch 188 Training Loss: 0.6709, Training Accuracy: 94.44%\n",
      "Epoch 188 Validation Loss: 0.7543, Validation Accuracy: 88.89%\n",
      "Epoch 189/2000\n",
      "Epoch 189 Training Loss: 0.6702, Training Accuracy: 95.14%\n",
      "Epoch 189 Validation Loss: 0.7675, Validation Accuracy: 83.33%\n",
      "Epoch 190/2000\n",
      "Epoch 190 Training Loss: 0.6696, Training Accuracy: 94.44%\n",
      "Epoch 190 Validation Loss: 0.7530, Validation Accuracy: 94.44%\n",
      "Epoch 191/2000\n",
      "Epoch 191 Training Loss: 0.6683, Training Accuracy: 95.83%\n",
      "Epoch 191 Validation Loss: 0.7638, Validation Accuracy: 88.89%\n",
      "Epoch 192/2000\n",
      "Epoch 192 Training Loss: 0.6664, Training Accuracy: 95.14%\n",
      "Epoch 192 Validation Loss: 0.7535, Validation Accuracy: 88.89%\n",
      "Epoch 193/2000\n",
      "Epoch 193 Training Loss: 0.6648, Training Accuracy: 95.14%\n",
      "Epoch 193 Validation Loss: 0.7549, Validation Accuracy: 88.89%\n",
      "Epoch 194/2000\n",
      "Epoch 194 Training Loss: 0.6640, Training Accuracy: 95.14%\n",
      "Epoch 194 Validation Loss: 0.7626, Validation Accuracy: 83.33%\n",
      "Epoch 195/2000\n",
      "Epoch 195 Training Loss: 0.6634, Training Accuracy: 95.14%\n",
      "Epoch 195 Validation Loss: 0.7494, Validation Accuracy: 88.89%\n",
      "Epoch 196/2000\n",
      "Epoch 196 Training Loss: 0.6623, Training Accuracy: 95.83%\n",
      "Epoch 196 Validation Loss: 0.7649, Validation Accuracy: 83.33%\n",
      "Epoch 197/2000\n",
      "Epoch 197 Training Loss: 0.6610, Training Accuracy: 95.83%\n",
      "Epoch 197 Validation Loss: 0.7517, Validation Accuracy: 88.89%\n",
      "Epoch 198/2000\n",
      "Epoch 198 Training Loss: 0.6596, Training Accuracy: 95.14%\n",
      "Epoch 198 Validation Loss: 0.7544, Validation Accuracy: 88.89%\n",
      "Epoch 199/2000\n",
      "Epoch 199 Training Loss: 0.6584, Training Accuracy: 95.83%\n",
      "Epoch 199 Validation Loss: 0.7617, Validation Accuracy: 83.33%\n",
      "Epoch 200/2000\n",
      "Epoch 200 Training Loss: 0.6577, Training Accuracy: 95.83%\n",
      "Epoch 200 Validation Loss: 0.7464, Validation Accuracy: 88.89%\n",
      "Epoch 201/2000\n",
      "Epoch 201 Training Loss: 0.6570, Training Accuracy: 96.53%\n",
      "Epoch 201 Validation Loss: 0.7621, Validation Accuracy: 83.33%\n",
      "Epoch 202/2000\n",
      "Epoch 202 Training Loss: 0.6558, Training Accuracy: 95.83%\n",
      "Epoch 202 Validation Loss: 0.7482, Validation Accuracy: 88.89%\n",
      "Epoch 203/2000\n",
      "Epoch 203 Training Loss: 0.6546, Training Accuracy: 96.53%\n",
      "Epoch 203 Validation Loss: 0.7520, Validation Accuracy: 88.89%\n",
      "Epoch 204/2000\n",
      "Epoch 204 Training Loss: 0.6534, Training Accuracy: 95.83%\n",
      "Epoch 204 Validation Loss: 0.7537, Validation Accuracy: 88.89%\n",
      "Epoch 205/2000\n",
      "Epoch 205 Training Loss: 0.6523, Training Accuracy: 95.83%\n",
      "Epoch 205 Validation Loss: 0.7482, Validation Accuracy: 88.89%\n",
      "Epoch 206/2000\n",
      "Epoch 206 Training Loss: 0.6512, Training Accuracy: 95.83%\n",
      "Epoch 206 Validation Loss: 0.7541, Validation Accuracy: 83.33%\n",
      "Epoch 207/2000\n",
      "Epoch 207 Training Loss: 0.6503, Training Accuracy: 95.83%\n",
      "Epoch 207 Validation Loss: 0.7497, Validation Accuracy: 88.89%\n",
      "Epoch 208/2000\n",
      "Epoch 208 Training Loss: 0.6496, Training Accuracy: 96.53%\n",
      "Epoch 208 Validation Loss: 0.7565, Validation Accuracy: 83.33%\n",
      "Epoch 209/2000\n",
      "Epoch 209 Training Loss: 0.6489, Training Accuracy: 95.83%\n",
      "Epoch 209 Validation Loss: 0.7438, Validation Accuracy: 88.89%\n",
      "Epoch 210/2000\n",
      "Epoch 210 Training Loss: 0.6482, Training Accuracy: 96.53%\n",
      "Epoch 210 Validation Loss: 0.7653, Validation Accuracy: 83.33%\n",
      "Epoch 211/2000\n",
      "Epoch 211 Training Loss: 0.6476, Training Accuracy: 95.83%\n",
      "Epoch 211 Validation Loss: 0.7393, Validation Accuracy: 94.44%\n",
      "Epoch 212/2000\n",
      "Epoch 212 Training Loss: 0.6468, Training Accuracy: 97.22%\n",
      "Epoch 212 Validation Loss: 0.7540, Validation Accuracy: 83.33%\n",
      "Epoch 213/2000\n",
      "Epoch 213 Training Loss: 0.6450, Training Accuracy: 95.83%\n",
      "Epoch 213 Validation Loss: 0.7453, Validation Accuracy: 88.89%\n",
      "Epoch 214/2000\n",
      "Epoch 214 Training Loss: 0.6434, Training Accuracy: 96.53%\n",
      "Epoch 214 Validation Loss: 0.7410, Validation Accuracy: 88.89%\n",
      "Epoch 215/2000\n",
      "Epoch 215 Training Loss: 0.6425, Training Accuracy: 96.53%\n",
      "Epoch 215 Validation Loss: 0.7516, Validation Accuracy: 83.33%\n",
      "Epoch 216/2000\n",
      "Epoch 216 Training Loss: 0.6419, Training Accuracy: 95.83%\n",
      "Epoch 216 Validation Loss: 0.7404, Validation Accuracy: 88.89%\n",
      "Epoch 217/2000\n",
      "Epoch 217 Training Loss: 0.6414, Training Accuracy: 97.22%\n",
      "Epoch 217 Validation Loss: 0.7524, Validation Accuracy: 83.33%\n",
      "Epoch 218/2000\n",
      "Epoch 218 Training Loss: 0.6404, Training Accuracy: 95.83%\n",
      "Epoch 218 Validation Loss: 0.7423, Validation Accuracy: 83.33%\n",
      "Epoch 219/2000\n",
      "Epoch 219 Training Loss: 0.6390, Training Accuracy: 97.22%\n",
      "Epoch 219 Validation Loss: 0.7509, Validation Accuracy: 83.33%\n",
      "Epoch 220/2000\n",
      "Epoch 220 Training Loss: 0.6376, Training Accuracy: 95.83%\n",
      "Epoch 220 Validation Loss: 0.7467, Validation Accuracy: 83.33%\n",
      "Epoch 221/2000\n",
      "Epoch 221 Training Loss: 0.6366, Training Accuracy: 97.22%\n",
      "Epoch 221 Validation Loss: 0.7455, Validation Accuracy: 77.78%\n",
      "Epoch 222/2000\n",
      "Epoch 222 Training Loss: 0.6359, Training Accuracy: 97.22%\n",
      "Epoch 222 Validation Loss: 0.7547, Validation Accuracy: 83.33%\n",
      "Epoch 223/2000\n",
      "Epoch 223 Training Loss: 0.6353, Training Accuracy: 96.53%\n",
      "Epoch 223 Validation Loss: 0.7388, Validation Accuracy: 88.89%\n",
      "Epoch 224/2000\n",
      "Epoch 224 Training Loss: 0.6352, Training Accuracy: 97.92%\n",
      "Epoch 224 Validation Loss: 0.7618, Validation Accuracy: 83.33%\n",
      "Epoch 225/2000\n",
      "Epoch 225 Training Loss: 0.6350, Training Accuracy: 97.22%\n",
      "Epoch 225 Validation Loss: 0.7349, Validation Accuracy: 88.89%\n",
      "Epoch 226/2000\n",
      "Epoch 226 Training Loss: 0.6351, Training Accuracy: 97.92%\n",
      "Epoch 226 Validation Loss: 0.7546, Validation Accuracy: 83.33%\n",
      "Epoch 227/2000\n",
      "Epoch 227 Training Loss: 0.6330, Training Accuracy: 97.22%\n",
      "Epoch 227 Validation Loss: 0.7432, Validation Accuracy: 83.33%\n",
      "Epoch 228/2000\n",
      "Epoch 228 Training Loss: 0.6309, Training Accuracy: 97.92%\n",
      "Epoch 228 Validation Loss: 0.7347, Validation Accuracy: 88.89%\n",
      "Epoch 229/2000\n",
      "Epoch 229 Training Loss: 0.6301, Training Accuracy: 97.92%\n",
      "Epoch 229 Validation Loss: 0.7500, Validation Accuracy: 83.33%\n",
      "Epoch 230/2000\n",
      "Epoch 230 Training Loss: 0.6303, Training Accuracy: 97.92%\n",
      "Epoch 230 Validation Loss: 0.7356, Validation Accuracy: 88.89%\n",
      "Epoch 231/2000\n",
      "Epoch 231 Training Loss: 0.6296, Training Accuracy: 97.92%\n",
      "Epoch 231 Validation Loss: 0.7416, Validation Accuracy: 83.33%\n",
      "Epoch 232/2000\n",
      "Epoch 232 Training Loss: 0.6276, Training Accuracy: 98.61%\n",
      "Epoch 232 Validation Loss: 0.7359, Validation Accuracy: 77.78%\n",
      "Epoch 233/2000\n",
      "Epoch 233 Training Loss: 0.6264, Training Accuracy: 98.61%\n",
      "Epoch 233 Validation Loss: 0.7347, Validation Accuracy: 83.33%\n",
      "Epoch 234/2000\n",
      "Epoch 234 Training Loss: 0.6263, Training Accuracy: 98.61%\n",
      "Epoch 234 Validation Loss: 0.7478, Validation Accuracy: 83.33%\n",
      "Epoch 235/2000\n",
      "Epoch 235 Training Loss: 0.6259, Training Accuracy: 98.61%\n",
      "Epoch 235 Validation Loss: 0.7290, Validation Accuracy: 88.89%\n",
      "Epoch 236/2000\n",
      "Epoch 236 Training Loss: 0.6248, Training Accuracy: 98.61%\n",
      "Epoch 236 Validation Loss: 0.7390, Validation Accuracy: 77.78%\n",
      "Epoch 237/2000\n",
      "Epoch 237 Training Loss: 0.6234, Training Accuracy: 100.00%\n",
      "Epoch 237 Validation Loss: 0.7367, Validation Accuracy: 83.33%\n",
      "Epoch 238/2000\n",
      "Epoch 238 Training Loss: 0.6227, Training Accuracy: 99.31%\n",
      "Epoch 238 Validation Loss: 0.7272, Validation Accuracy: 83.33%\n",
      "Epoch 239/2000\n",
      "Epoch 239 Training Loss: 0.6224, Training Accuracy: 99.31%\n",
      "Epoch 239 Validation Loss: 0.7415, Validation Accuracy: 83.33%\n",
      "Epoch 240/2000\n",
      "Epoch 240 Training Loss: 0.6218, Training Accuracy: 100.00%\n",
      "Epoch 240 Validation Loss: 0.7265, Validation Accuracy: 88.89%\n",
      "Epoch 241/2000\n",
      "Epoch 241 Training Loss: 0.6208, Training Accuracy: 100.00%\n",
      "Epoch 241 Validation Loss: 0.7303, Validation Accuracy: 83.33%\n",
      "Epoch 242/2000\n",
      "Epoch 242 Training Loss: 0.6198, Training Accuracy: 99.31%\n",
      "Epoch 242 Validation Loss: 0.7320, Validation Accuracy: 77.78%\n",
      "Epoch 243/2000\n",
      "Epoch 243 Training Loss: 0.6189, Training Accuracy: 100.00%\n",
      "Epoch 243 Validation Loss: 0.7275, Validation Accuracy: 83.33%\n",
      "Epoch 244/2000\n",
      "Epoch 244 Training Loss: 0.6183, Training Accuracy: 100.00%\n",
      "Epoch 244 Validation Loss: 0.7341, Validation Accuracy: 83.33%\n",
      "Epoch 245/2000\n",
      "Epoch 245 Training Loss: 0.6179, Training Accuracy: 100.00%\n",
      "Epoch 245 Validation Loss: 0.7261, Validation Accuracy: 83.33%\n",
      "Epoch 246/2000\n",
      "Epoch 246 Training Loss: 0.6175, Training Accuracy: 100.00%\n",
      "Epoch 246 Validation Loss: 0.7367, Validation Accuracy: 83.33%\n",
      "Epoch 247/2000\n",
      "Epoch 247 Training Loss: 0.6170, Training Accuracy: 100.00%\n",
      "Epoch 247 Validation Loss: 0.7210, Validation Accuracy: 83.33%\n",
      "Epoch 248/2000\n",
      "Epoch 248 Training Loss: 0.6164, Training Accuracy: 100.00%\n",
      "Epoch 248 Validation Loss: 0.7344, Validation Accuracy: 83.33%\n",
      "Epoch 249/2000\n",
      "Epoch 249 Training Loss: 0.6153, Training Accuracy: 100.00%\n",
      "Epoch 249 Validation Loss: 0.7228, Validation Accuracy: 83.33%\n",
      "Epoch 250/2000\n",
      "Epoch 250 Training Loss: 0.6142, Training Accuracy: 100.00%\n",
      "Epoch 250 Validation Loss: 0.7203, Validation Accuracy: 88.89%\n",
      "Epoch 251/2000\n",
      "Epoch 251 Training Loss: 0.6135, Training Accuracy: 100.00%\n",
      "Epoch 251 Validation Loss: 0.7270, Validation Accuracy: 83.33%\n",
      "Epoch 252/2000\n",
      "Epoch 252 Training Loss: 0.6131, Training Accuracy: 100.00%\n",
      "Epoch 252 Validation Loss: 0.7190, Validation Accuracy: 83.33%\n",
      "Epoch 253/2000\n",
      "Epoch 253 Training Loss: 0.6128, Training Accuracy: 100.00%\n",
      "Epoch 253 Validation Loss: 0.7302, Validation Accuracy: 83.33%\n",
      "Epoch 254/2000\n",
      "Epoch 254 Training Loss: 0.6126, Training Accuracy: 100.00%\n",
      "Epoch 254 Validation Loss: 0.7167, Validation Accuracy: 88.89%\n",
      "Epoch 255/2000\n",
      "Epoch 255 Training Loss: 0.6122, Training Accuracy: 100.00%\n",
      "Epoch 255 Validation Loss: 0.7289, Validation Accuracy: 83.33%\n",
      "Epoch 256/2000\n",
      "Epoch 256 Training Loss: 0.6114, Training Accuracy: 100.00%\n",
      "Epoch 256 Validation Loss: 0.7139, Validation Accuracy: 88.89%\n",
      "Epoch 257/2000\n",
      "Epoch 257 Training Loss: 0.6104, Training Accuracy: 100.00%\n",
      "Epoch 257 Validation Loss: 0.7197, Validation Accuracy: 88.89%\n",
      "Epoch 258/2000\n",
      "Epoch 258 Training Loss: 0.6093, Training Accuracy: 100.00%\n",
      "Epoch 258 Validation Loss: 0.7202, Validation Accuracy: 83.33%\n",
      "Epoch 259/2000\n",
      "Epoch 259 Training Loss: 0.6088, Training Accuracy: 100.00%\n",
      "Epoch 259 Validation Loss: 0.7112, Validation Accuracy: 88.89%\n",
      "Epoch 260/2000\n",
      "Epoch 260 Training Loss: 0.6087, Training Accuracy: 100.00%\n",
      "Epoch 260 Validation Loss: 0.7238, Validation Accuracy: 83.33%\n",
      "Epoch 261/2000\n",
      "Epoch 261 Training Loss: 0.6082, Training Accuracy: 100.00%\n",
      "Epoch 261 Validation Loss: 0.7137, Validation Accuracy: 88.89%\n",
      "Epoch 262/2000\n",
      "Epoch 262 Training Loss: 0.6075, Training Accuracy: 100.00%\n",
      "Epoch 262 Validation Loss: 0.7175, Validation Accuracy: 83.33%\n",
      "Epoch 263/2000\n",
      "Epoch 263 Training Loss: 0.6066, Training Accuracy: 100.00%\n",
      "Epoch 263 Validation Loss: 0.7123, Validation Accuracy: 88.89%\n",
      "Epoch 264/2000\n",
      "Epoch 264 Training Loss: 0.6058, Training Accuracy: 100.00%\n",
      "Epoch 264 Validation Loss: 0.7136, Validation Accuracy: 88.89%\n",
      "Epoch 265/2000\n",
      "Epoch 265 Training Loss: 0.6053, Training Accuracy: 100.00%\n",
      "Epoch 265 Validation Loss: 0.7174, Validation Accuracy: 83.33%\n",
      "Epoch 266/2000\n",
      "Epoch 266 Training Loss: 0.6049, Training Accuracy: 100.00%\n",
      "Epoch 266 Validation Loss: 0.7098, Validation Accuracy: 88.89%\n",
      "Epoch 267/2000\n",
      "Epoch 267 Training Loss: 0.6046, Training Accuracy: 100.00%\n",
      "Epoch 267 Validation Loss: 0.7208, Validation Accuracy: 83.33%\n",
      "Epoch 268/2000\n",
      "Epoch 268 Training Loss: 0.6042, Training Accuracy: 100.00%\n",
      "Epoch 268 Validation Loss: 0.7079, Validation Accuracy: 94.44%\n",
      "Epoch 269/2000\n",
      "Epoch 269 Training Loss: 0.6037, Training Accuracy: 100.00%\n",
      "Epoch 269 Validation Loss: 0.7166, Validation Accuracy: 83.33%\n",
      "Epoch 270/2000\n",
      "Epoch 270 Training Loss: 0.6031, Training Accuracy: 100.00%\n",
      "Epoch 270 Validation Loss: 0.7089, Validation Accuracy: 88.89%\n",
      "Epoch 271/2000\n",
      "Epoch 271 Training Loss: 0.6024, Training Accuracy: 100.00%\n",
      "Epoch 271 Validation Loss: 0.7114, Validation Accuracy: 88.89%\n",
      "Epoch 272/2000\n",
      "Epoch 272 Training Loss: 0.6017, Training Accuracy: 100.00%\n",
      "Epoch 272 Validation Loss: 0.7084, Validation Accuracy: 88.89%\n",
      "Epoch 273/2000\n",
      "Epoch 273 Training Loss: 0.6011, Training Accuracy: 100.00%\n",
      "Epoch 273 Validation Loss: 0.7086, Validation Accuracy: 88.89%\n",
      "Epoch 274/2000\n",
      "Epoch 274 Training Loss: 0.6007, Training Accuracy: 100.00%\n",
      "Epoch 274 Validation Loss: 0.7122, Validation Accuracy: 88.89%\n",
      "Epoch 275/2000\n",
      "Epoch 275 Training Loss: 0.6003, Training Accuracy: 100.00%\n",
      "Epoch 275 Validation Loss: 0.7042, Validation Accuracy: 88.89%\n",
      "Epoch 276/2000\n",
      "Epoch 276 Training Loss: 0.6001, Training Accuracy: 100.00%\n",
      "Epoch 276 Validation Loss: 0.7163, Validation Accuracy: 83.33%\n",
      "Epoch 277/2000\n",
      "Epoch 277 Training Loss: 0.5999, Training Accuracy: 100.00%\n",
      "Epoch 277 Validation Loss: 0.7021, Validation Accuracy: 94.44%\n",
      "Epoch 278/2000\n",
      "Epoch 278 Training Loss: 0.6000, Training Accuracy: 100.00%\n",
      "Epoch 278 Validation Loss: 0.7187, Validation Accuracy: 83.33%\n",
      "Epoch 279/2000\n",
      "Epoch 279 Training Loss: 0.5999, Training Accuracy: 100.00%\n",
      "Epoch 279 Validation Loss: 0.7003, Validation Accuracy: 94.44%\n",
      "Epoch 280/2000\n",
      "Epoch 280 Training Loss: 0.5992, Training Accuracy: 100.00%\n",
      "Epoch 280 Validation Loss: 0.7079, Validation Accuracy: 88.89%\n",
      "Epoch 281/2000\n",
      "Epoch 281 Training Loss: 0.5979, Training Accuracy: 100.00%\n",
      "Epoch 281 Validation Loss: 0.7019, Validation Accuracy: 88.89%\n",
      "Epoch 282/2000\n",
      "Epoch 282 Training Loss: 0.5971, Training Accuracy: 100.00%\n",
      "Epoch 282 Validation Loss: 0.6982, Validation Accuracy: 94.44%\n",
      "Epoch 283/2000\n",
      "Epoch 283 Training Loss: 0.5972, Training Accuracy: 100.00%\n",
      "Epoch 283 Validation Loss: 0.7097, Validation Accuracy: 83.33%\n",
      "Epoch 284/2000\n",
      "Epoch 284 Training Loss: 0.5972, Training Accuracy: 100.00%\n",
      "Epoch 284 Validation Loss: 0.6975, Validation Accuracy: 94.44%\n",
      "Epoch 285/2000\n",
      "Epoch 285 Training Loss: 0.5965, Training Accuracy: 100.00%\n",
      "Epoch 285 Validation Loss: 0.7069, Validation Accuracy: 88.89%\n",
      "Epoch 286/2000\n",
      "Epoch 286 Training Loss: 0.5956, Training Accuracy: 100.00%\n",
      "Epoch 286 Validation Loss: 0.7018, Validation Accuracy: 88.89%\n",
      "Epoch 287/2000\n",
      "Epoch 287 Training Loss: 0.5949, Training Accuracy: 100.00%\n",
      "Epoch 287 Validation Loss: 0.6964, Validation Accuracy: 94.44%\n",
      "Epoch 288/2000\n",
      "Epoch 288 Training Loss: 0.5948, Training Accuracy: 100.00%\n",
      "Epoch 288 Validation Loss: 0.7074, Validation Accuracy: 88.89%\n",
      "Epoch 289/2000\n",
      "Epoch 289 Training Loss: 0.5947, Training Accuracy: 100.00%\n",
      "Epoch 289 Validation Loss: 0.6957, Validation Accuracy: 94.44%\n",
      "Epoch 290/2000\n",
      "Epoch 290 Training Loss: 0.5943, Training Accuracy: 100.00%\n",
      "Epoch 290 Validation Loss: 0.7064, Validation Accuracy: 88.89%\n",
      "Epoch 291/2000\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Example of training and validation for a number of epochs\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train_loss, train_accuracy = train_model(train_loader, model, criterion, optimizer, device)\n",
    "    print(f'Epoch {epoch+1} Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_accuracy = validate_model(val_loader, model, criterion, device)\n",
    "    print(f'Epoch {epoch+1} Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Test the model after training is completed\n",
    "test_loss, test_accuracy = test_model(test_loader, model, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c32ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(number_of_classes, shape).to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342eb42",
   "metadata": {},
   "source": [
    "### Rep Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rep_Counter(file_array, index, output_dir, log_output_filename, model, device):\n",
    "    log = open(output_dir + log_output_filename, \"a\", encoding=\"utf-8\")\n",
    "    print(\"#############################################################\", file=log)\n",
    "    filename = Path(file_array[index])\n",
    "\n",
    "    default_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    class_colours = [] \n",
    "    random.seed(seed)\n",
    "    for i in class_names:\n",
    "        B = random.randint(0, 255)\n",
    "        G = random.randint(0, 255)\n",
    "        R = random.randint(0, 255)\n",
    "        colour = tuple([B, G, R])\n",
    "        class_colours.append(colour)\n",
    "        \n",
    "    sequence = []\n",
    "    logger = [] #sequential log, gives exact order\n",
    "    predictions = []\n",
    "    \n",
    "    graph_data = [[] for i in range(0, len(class_names))]\n",
    "    \n",
    "    threshold = 0.99\n",
    "    count = 1\n",
    "    cap = cv2.VideoCapture(file_array[index])\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(output_dir + filename.name, fourcc, fps, (width, height))\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    with mp_pose.Pose(min_detection_confidence=min_detect, min_tracking_confidence=min_track,\n",
    "                      model_complexity=model_complex, enable_segmentation=segmentation) as pose:\n",
    "        while cap.isOpened():\n",
    "            number_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            count += 1\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            exact_time_in_seconds = round(count/fps, 2)\n",
    "\n",
    "            # Extract keypoints using your custom function\n",
    "            image, keypoints = extract_keypoints(frame, pose)\n",
    "            \n",
    "            if keypoints is None:\n",
    "                continue\n",
    "            \n",
    "            sequence.append(keypoints)\n",
    "            sequence_np = np.array(sequence)\n",
    "            sequence_np = scaler.fit_transform(sequence_np)\n",
    "            sequence_np = pad_array(sequence_np, shape[1:])\n",
    "            sequence_tensor = torch.tensor(sequence_np, dtype=torch.float32).unsqueeze(0)\n",
    "            sequence_tensor = sequence_tensor.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output = model(sequence_tensor)\n",
    "                result = torch.nn.functional.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "            for i in range(len(class_names)):\n",
    "                graph_data[i].append(result[i])\n",
    "            \n",
    "            predicted_class_index = np.argmax(result)                 \n",
    "            predicted_class_percentage = result[predicted_class_index]\n",
    "            predicted_class_percentage_rounded = round(predicted_class_percentage, 2)\n",
    "            predicted_class_name = class_names[predicted_class_index]\n",
    "            predictions.append(predicted_class_index)\n",
    "\n",
    "            if np.unique(predictions[-10:])[0] == predicted_class_index:\n",
    "                if predicted_class_percentage > threshold:   \n",
    "                    if len(logger) > 0:\n",
    "                        if predicted_class_name != logger[-1][0]:\n",
    "                            logger.append([predicted_class_name, exact_time_in_seconds, predicted_class_percentage_rounded])\n",
    "                    else:\n",
    "                        logger.append([predicted_class_name, exact_time_in_seconds, predicted_class_percentage_rounded])\n",
    "\n",
    "            # Probability boxes for each class\n",
    "            for class_index, probability in enumerate(result):\n",
    "                start_point = (0, 60 + class_index * 40)\n",
    "                end_point = (int(probability * 100), 90 + class_index * 40)\n",
    "                colour = class_colours[class_index]\n",
    "                thickness = -1\n",
    "                cv2.rectangle(image, start_point, end_point, colour, thickness)\n",
    "\n",
    "                text = \"{}:{}%\".format(class_names[class_index], int(probability*100))\n",
    "                org = (0, 85 + class_index * 40)\n",
    "                font_scale = 1\n",
    "                colour = (255,255,255)\n",
    "                thickness = 2\n",
    "                line_type = cv2.LINE_AA\n",
    "                cv2.putText(image, text, org, default_font, font_scale, colour, thickness, line_type)\n",
    "            \n",
    "            # Repetition count\n",
    "            move_count_array = [0] * len(class_names)\n",
    "            if len(logger) > 0:\n",
    "                for log_value in logger:\n",
    "                    index_to_increment = label_map.get(log_value[0])\n",
    "                    move_count_array[index_to_increment] += 1\n",
    "\n",
    "            # Display logger text\n",
    "            text = \"{}\".format(logger)\n",
    "            org = (3, 30)\n",
    "            font_scale = 0.6\n",
    "            colour = (255,255,255)\n",
    "            thickness = 2\n",
    "            line_type = cv2.LINE_AA\n",
    "            cv2.putText(image, text, org, default_font, font_scale, colour, thickness, line_type)\n",
    "            cv2.imshow(\"Rep-Tracker\", image)\n",
    "            out.write(image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        count_array = [predictions.count(i) for i in range(len(class_names))]\n",
    "        \n",
    "        # Log output\n",
    "        print(\"Video {}, {}\".format(index+1,filename.name), file=log)\n",
    "        print(\"###Repetition Counter###\", file=log)\n",
    "        for i in range(len(class_names)):\n",
    "            print(\"{}={}\".format(class_names[i], move_count_array[i]), file=log)\n",
    "            \n",
    "        print(\"###Frames###\", file=log)\n",
    "        for i in range(len(class_names)):\n",
    "            print(\"{}={}\".format(class_names[i], count_array[i]), file=log)\n",
    "        \n",
    "        print(\"###Logger###\", file=log)\n",
    "        print(str(logger), file=log)\n",
    "            \n",
    "        print(\"Readable Frames: {}/{}\".format(len(graph_data[0]), number_of_frames), file=log)\n",
    "        print(\"Estimate: \\\"{}\\\" with {}/{} frames.\".format(class_names[np.argmax(count_array)], np.max(count_array), number_of_frames), file=log)\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Plotting graph\n",
    "    plt.figure(figsize=(20, 16))    \n",
    "    for i in range(len(class_names)):\n",
    "        plt.plot(graph_data[i], label=class_names[i])\n",
    "        \n",
    "    plt.title(\"Graph for {}\".format(filename.name))\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(output_dir+filename.stem)\n",
    "    return np.argmax(count_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_counter_accuracy(file_list, log_output_filename):\n",
    "    results = []\n",
    "    output = os.getcwd() + r\"/results/\"\n",
    "    \n",
    "    if not os.path.exists(output):\n",
    "        os.mkdir(output)\n",
    "    \n",
    "    for index in range(len(file_list)):\n",
    "        results.append(Rep_Counter(test_files, index, output, log_output_filename, model, device))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7674f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(labels, outputs):\n",
    "    count = 0\n",
    "    for index in range(len(labels)):\n",
    "        if labels[index] == outputs[index]:\n",
    "            count += 1\n",
    "    return (count/len(labels))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95c8f9",
   "metadata": {},
   "source": [
    "### Rep Counter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe757a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(y_test)\n",
    "accuracy_array = rep_counter_accuracy(test_files, str(datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_percentage = similarity(labels, accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(accuracy_array)\n",
    "print(similarity_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(labels, accuracy_array)\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names).set(title=\"Confusion Matrix of Frame Prediction Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
